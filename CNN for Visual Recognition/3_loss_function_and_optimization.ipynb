{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Loss function helps to quantify, for any given W, how good it is.\n",
    "\n",
    "- Optmization procedure looks for the less bad W.\n",
    "\n",
    "- Loss over the dataset is a sum of loss over examples (average).\n",
    "\n",
    "- Multiclass SVM loss (Hinge loss): summation over all categories different from the true one(max(0, category_i_score - correct_category_score + 1)). The correct class is omitted so that the loss could reach 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization term is added to prevent overfitting. The model should be simple so it works on test data. The loss function is now composed by data loss + regularization loss.\n",
    "\n",
    "- The hyperparameter lambda (regularization strength) trades off between data loss and regularization loss (fitting the training data vs. preferring simpler models).\n",
    "\n",
    "- More complex models (Ws) are preferred only when the data loss is very low (because the regularization loss would be very high).\n",
    "\n",
    "- L2 regularization (weight decay) measures complexity according to how spread are the values in W. It prefers sparse solutions because they are less complex solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multinomial Logistic Regression: Softmax classifier. The result is a probability distribution.\n",
    "\n",
    "- The loss is called 'cross-entropy'. It is calculated as the negative log of the true class' predicted probability. The minus is there because maximizing the probability means minimizing the log of it (monotonic function).\n",
    "\n",
    "- Optimization is used for minimizing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient: slope in the multi-variable setting. Vector of partial derivatives along each dimension that points in the direction of the greatest increase of a function.\n",
    "\n",
    "- Negative gradient points in the direction of steepest descent.\n",
    "\n",
    "- The slope of the function in any direction is equal to the dot product between the gradient and the unit vector describing that direction.\n",
    "\n",
    "- Each element in the gradient will tell us how much will the loss change if we move a tiny infinitesimal amount in that coordinate direction.\n",
    "\n",
    "- The analytic gradient is used because it is exact and fast.\n",
    "\n",
    "- Gradient Descent (parameter update): W - (learning_rate * gradient).\n",
    "\n",
    "- Stochastic Gradient Descent (SGD): mini-batch sampling of the train set for computing an estimate of the true gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before feeding the linear classifier with raw pixels, it is better to compute various feature representations of the images (quantities related to their appearance concatenated).\n",
    "\n",
    "- The image feature transformation may become linearly separable whereas the raw representation is not.\n",
    "\n",
    "- Feature vector representations: color histogram, Histogram of Oriented Edges (measures the local orientation of edges i.e. the dominant edges in different regions).\n",
    "\n",
    "- Visual words (\"codebooks\": samples of patches of the images that were clustered and averaged to form patterns. Then, a histogram of codebooks could be generated over any image.\n",
    "\n",
    "- CNN: the features does not need to be extracted ahead of time. It learns the features directly from the data using convolutional layers. The optimization occurs for every parameter of the network, including the ones related to the convolutions. It means that the extracted features do not stay fixed as in the traditional feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}