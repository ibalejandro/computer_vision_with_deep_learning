{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Convolutional layers try to mantain spatial structure.\n",
    "\n",
    "- Nearby cells in cortex represent nearby regions in the visual field.\n",
    "\n",
    "- Pooling is used so that the modifiable parameters remain invariant to different minor modifications from the simple cells.\n",
    "\n",
    "- CNNs are used for classification, retrieval, detection (ROI), segmentation, face recognition, pose recognition, reinforcement learning, interpretation and diagnosis for medical images, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of stretching the image into one long vector, we are going to keep the 3D structure of it.\n",
    "\n",
    "- Convolve the filter with the image: slide over the image spatially computing dot products. The patch of the image (including its three dimensions) is streched out into a single vector as well as the filter, the dot product is calculated and one single number is generated.\n",
    "\n",
    "- Filters always extend the full depth of the input volume (channels).\n",
    "\n",
    "- The result after sliding the filter over the image is calles \"activation map\".\n",
    "\n",
    "- A convolutional layer generates multiple filters and stacks them up, where each of them is looking for a template in the original input volume.\n",
    "\n",
    "- CNN is a sequence of convolutional layers, interspersed with activation functions (CONV-ReLU --> CONV-ReLU --> CONV-ReLU ...).\n",
    "\n",
    "- What these stacked features end up being, is simple to more complex features.\n",
    "\n",
    "- Each element of the result of a convolution represents what in the input would look like that maximizes the activation of the neuron (what is ththe neuron looking for?).\n",
    "\n",
    "- The result of using a filter over a patch of the image and then activating it will have more white values on the matching points. Those white regions indicate the presence of the filter on the image.\n",
    "\n",
    "- Convolution standard definition: element-wise multiplication and sum of a filter and the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pooling layer: downsamples the size of the activation maps. Makes them smaller and more manageable.\n",
    "\n",
    "- Ouput size of a convolution: ((image_size - filter_size) / stride) + 1. The result must be a whole number so that the filter can be used.\n",
    "\n",
    "- Zero-padding: add a margin of zeros to the image for being able to use the desired stride.\n",
    "\n",
    "- In general: K (powers of 2) filters, stride of 1, filters of size FxF (3x3, 5x5, 7x7) and zero padding with (F-1) / 2 in order to preserve size spatially. Shrinking the volumes spatially too fast doesn't work well in practice (loss of information).\n",
    "\n",
    "- Generic output volume size: (((image_size + (paddind * 2)) - filter_size) / stride) + 1.\n",
    "\n",
    "- A filter has a set of parameters for each dimension of the input data plus one more for the general bias. Thus, a 5x5 filter over an RGB image has 25*3+1 = 76 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The neurons have local connectivity now instead of being connected to the entire input. They are just looking at a local region spatially of the image.\n",
    "\n",
    "- There are K neurons (where K is the number of filters appplied on a layer) looking to the same spatial location of the input representation of the image and only to it, but searching different things (\"different filters applied to the same spatial location\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are neurons that share the same parameters due to the fact that there are K neurons for each spatial location of the image and one of these neurons has the parameters for one of the K filters. Then, considering that the parameters of a filter don't change according to the spatial location, there will be, for instance, two neurons (one in location a and one in location b) sharing the same parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pooling works in a non-overlapping manner and affects the size of the activation maps spatially but not the depth. Common settings are 2x2, 3x3.\n",
    "\n",
    "- Max pooling: preserves the maximum value of every patch of an activation map.\n",
    "\n",
    "- It is not common to use zero-padding for Pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After the last convolutional layer, the result after passing all convolutions is stretched out into a 1-D vector in order to aggregate everything. Then, this is connected to a FC-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}