{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Computational Graph: the nodes are steps of computation that we go through.\n",
    "\n",
    "- Backpropagation: recursively uses the chain rule in order to compute the gradient with respect to every variable in the computational graph. It works backwards and computes each gradient along the way using the principle of intermediate (indirect) connections. Each node is only aware of its immediate surroundings.\n",
    "\n",
    "- Chain rule: the effect of Y on F is equivalent to the effect of Q on F compounded with (multiplied by) the effect of Y on Q. In that way, it is not necessary to derive the entire expression.\n",
    "\n",
    "- Local gradient: the gradient of the output of a node with respect to the inputs going into that node.\n",
    "\n",
    "- Upstream gradient: the gradient of the loss with respect to the output of a node. The upstream gradient is a number at the end of the day.\n",
    "\n",
    "- In the chain rule, we always take the upstream gradient and multiply it by the local gradient to get the gradient with respect to the input. Then, this is what is sent back to the next connected nodes going backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can group any nodes that you want to make any sorts of more complex nodes, as long as you can write down the local gradient for this.\n",
    "\n",
    "- Add gate: gradient distributor.\n",
    "\n",
    "- Max gate: gradient router (to the one that is the maximum). It makes sense because only the value that was the maximum got passed down to the rest of the computational graph during the forward pass. That value affected the computation linearly (variation ratio of 1) whereas the other one did nothing (variation ratio of 0).\n",
    "\n",
    "- Multiplication gate: gradient switcher.\n",
    "\n",
    "- When multiple nodes are connected to a single node backwards, the gradients add up at this single node. The upstream gradient would be the sum of the upstream gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradients when using vectors are placed in the Jacobian matrix. It contains the derivative of each element of the output with respect to each element of the input.\n",
    "\n",
    "- L2 norm is the summation over all squared elements of W.\n",
    "\n",
    "- Each element of the gradient vector tells how much the particular element related to it on the orignal vector affects the final output of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QUESTION (46:05): are'nt df/dW1,1 and df/dW2,1 backwards according to the formula?\n",
    "\n",
    "- A/ Just a typo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using matrices and vectors multiplication, the effect of a particular element on the function is the summation over all its possible effects (over all the calculations that involve that particular element)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topologically order: process all the inputs of a node before processing that node.\n",
    "\n",
    "- It is necessary to cache the values of the forward pass because we end up using them in the backward pass.\n",
    "\n",
    "- Layers are basically computational nodes.\n",
    "\n",
    "- Margins: the vector of elements that conform the loss when they are summed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks stack linear layers in a hierarchical way and puts non-linearities between them in order to avoid it collapsing to a single linear function. As a result, a more complex non-linear function is made up.\n",
    "\n",
    "- The second layer in the hierarchy makes a weighted sum of all the templates in the first layer and determines the final score for each particular class. The in-between non-linearity tells the first scoring for the input (how much of each template is present) and then, the second linear layer weights all of these intermediate scores to get the final scores.\n",
    "\n",
    "- The non-linearities represent the firing rate of neurons.\n",
    "\n",
    "- The efficiency of neural networks is achieved through vectorized computation (matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}