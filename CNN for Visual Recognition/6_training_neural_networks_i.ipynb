{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Sigmoid function: squashes numbers to range [0, 1].\n",
    "\n",
    "- Problems of sigmoid:\n",
    "    - Very negative/positive values of X can make the gradient equals to 0.\n",
    "    - Outputs are not zero-centered, which gives very inefficient gradient updates as a result. The sign of the upstream gradient is never changed along the backpropagation. Zero-mean data is desired in order to have positive and negative x values.\n",
    "    - exp() is compute expensive.\n",
    "\n",
    "- tanh function: squashes numbers to range [-1, 1].\n",
    "\n",
    "- Problems of tanh:\n",
    "    - Very negative/positive values of X can make the gradient equals to 0.\n",
    "\n",
    "- ReLU function: f(x) = max(0, x).\n",
    "\n",
    "- Problems of ReLU:\n",
    "    - Very negative values of X can make the gradient equals to 0 (kills the gradient in half of the regime).\n",
    "    - Outputs are not zero-centered.\n",
    "\n",
    "- A too high learning rate is bad for convergence.\n",
    "\n",
    "- In practice, ReLU nerons are initialized with slightly positive biases in order to increase the likelihood of being active at initialization and get some updates.\n",
    "\n",
    "- Leaky ReLU: f(x) = max(0.01x, x). Instead of being flat in the negative regime, we are going to give a slight negative slope there.\n",
    "\n",
    "- Exponential Linear Units (ELU): Instead of being sloped in the negative regime like Leaky ReLU, you are building back in a negative saturation regime. This adds some robustness to noise.\n",
    "\n",
    "- Maxout neuron: does not have the basic form of dot product -> non-linearity. It generalizes ReLU and Leaky ReLU and works as max(w1*x + b1, w2*x + b2).\n",
    "\n",
    "- Problems of Maxout neuron:\n",
    "    - Doubles the number of parameters per neuron.\n",
    "\n",
    "- In general: use ReLU and be careful with the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data preprocessing: zero-mean and normalize by standard deviation (to put all features in the same range for achieving equal contribution).\n",
    "\n",
    "- For images, we do do the zero centering but we don't need to normalize. Generally, at each location you already comparable scale and distribution. The general mean can be substracted or also the mean per channel.\n",
    "\n",
    "- The mean to be susbtracted is calculated once at the beginning (before executing the training) and over the whole train set.\n",
    "\n",
    "- To project non-image data into a lower dimensional space of new features, PCA and Whitening are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All weights cannot be equally initialized because every neuron would behave the same. The update would be the same for all parameters and there would be no symmetry breaking.\n",
    "\n",
    "- Initializing the weights with a very low stddev make the values for activation collapse to zero very quickly and this negatively affects the updates. The gradient vanishes.\n",
    "\n",
    "- Initializing the weights with big values saturates almost all neurons. Thus, the gradients (flat slope) will be all zero and no update will occur.\n",
    "\n",
    "- Xavier initialization: standard Gaussian / number of inputs. We want the variance of the input to be the same as the variance of the output. But, when using ReLU, have of the variance is being lost. The solution is to initialize W with (standard Gaussian / (number of inputs / 2)), considering that half the neurons get killed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch normalization: compute the empirical mean and variance independently for each dimension of the input (current mini-batch) and then normalize using ((x - mean) / stddev).\n",
    "\n",
    "- Usually inserted after FC and before non-linearity. In CNN, we have one mean and one stddev per activation map and we are going to normalize by this across all the examples in the batch. Normalization over each dimension and per activation map.\n",
    "\n",
    "- In order to control the level of saturation on the non-linearities, an additional squashing and scaling operation is used. Scale by constant \"gamma\" and shift by factor \"beta\". This allows to recover the identitiy function partially or totally (as there was no batch normalization), because the network can learn \"gamma\" as stddev and \"beta\" as mean.\n",
    "\n",
    "- Batch normalization improves gradient flow through the network and reduces the strong dependence on initialization.\n",
    "\n",
    "- At test time, the mean and stddev are not computed based on the batch. Instead, a single fixed empirical mean and variance of activations during training is used (for example, with running averages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-validate learning rates in the range [1e-3, 1e-5].\n",
    "\n",
    "- It is better to optimize in log space (rates that are multiplied or divided by some value rather than uniformly sampled). For instance, 10**uniform(low, high).\n",
    "\n",
    "- Hyperparameters: network architecture, learning rate, its decay, update type, regularization (L2, Dropout strength).\n",
    "\n",
    "- The ratio of the norms between W and gradient should be somewhere around 0.001 (the updates cannot be too large [dominating] or to small [no effect] compared to the original values of the parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}