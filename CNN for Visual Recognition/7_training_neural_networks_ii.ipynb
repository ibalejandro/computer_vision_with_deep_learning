{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- If the data for activation has not zero-mean and unit variance, then small perturbations in the weight matrix of that layer of the network could cause large perturbations in the output of that layer. This makes learning more difficult.\n",
    "\n",
    "- Regularization is used to reduce the gap between train and test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If loss changes quickly in one direction and slowly in another, this result in very slow progress along shallow (poco profunda) dimension, jitter along steepest direction.\n",
    "\n",
    "- Saddle point: in one direction we go up and in the other direction we go down. These points are much more common in high dimensions. Local minima is very rare in high dimensions (every step in every direction must increase the value of the function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD+Momentum: we mantain the velovity over time and we add our gradient estimates to the velovity. Then, we step in the direction of the velocity rather than stepping in the direction of the gradient. Even if the gradient is 0 in a saddle point, we have velocity and can continue moving downwards. This overcomes the noise in a gradient estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx = 0\n",
    "vx = rho * vx + dx  # rho gives \"friction\"; typically 0.9.\n",
    "x += learning_rate * vx\n",
    "# This is a smooth moving average of your recent gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nesterov momentum: step in the direction of the velocity, evaluate the gradient at that point, go back and mix velocity and gradient to make the step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_v = v\n",
    "v = rho * v - learning_rate * dx\n",
    "# Incorporates a correcting term between the previous and current velocity and that minimizes the overshooting.\n",
    "x += -rho * old_v + (1 + rho) * v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaGrad: keep a running sum of all the squared gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_squared += dx * dx  \n",
    "x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSProp: momentum over squared gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx\n",
    "x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam: RMSProp with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_moment = 0\n",
    "second_moment = 0\n",
    "for t in range(num_iterations):\n",
    "    first_moment = beta_1 * first_moment + (1 - beta_1) * dx  # Momentum.\n",
    "    second_moment = beta_2 * second_moment + (1 - beta_2) * dx * dx  # RMSProp.\n",
    "    # Bias correction for the fact that first and second moment estimates start at zero.\n",
    "    first_unbias = first_moment / (1 - beta_1 ** t)\n",
    "    second_unbias = second_moment / (1 - beta_2 ** t)\n",
    "    x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)  # RMSProp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good starting point: Adam with beta_1 = 0.9, beta_2 = 0.99 and learning_rate = 1e-3 or 5e-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning rate decay over time: step, exponential, 1/t. It is useful when the loss is getting stuck in a plateau. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout: In each forward pass, randomly set the output of some neurons to zero (0.5 by default) at every layer. It forces the network to have a redundant representation and prevents co-adaptation of features.\n",
    "\n",
    "- At test time: scale the activations by the dropout probability.\n",
    "\n",
    "- Dropout is equivalent to Batch normalization for regularizing (add noise at train time and remove the average of it [marginalize] at test time) and maybe better because of the hyperparameter dropout_rate.\n",
    "\n",
    "- Data Augmentation: random mix of translation, rotation, stretching, distorsion, etc. for regularizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer learning: train on ImageNet, then freeze all the weights up to the final FC layer, reinitialize the matrix that has the weights going into the final FC layer randomly and finally, train only those parameters and let them convert to your small data. The original parameters of the network that converged on ImageNet work pretty well generally and they should be changed just a very small amount.\n",
    "\n",
    "- \"Model Zoo\" on DL-frameworks: pretrained models for different tasks so you don't need to train your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}